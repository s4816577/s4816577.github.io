<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content=".red {   color: #DC3724; }   Introduction &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;This thesis mainly develops a supervised absolute  camera pose learning framework, in particular scenes,  takes a single RGB image as">
<meta property="og:type" content="website">
<meta property="og:title" content="PoseAttention">
<meta property="og:url" content="http://example.com/PoseAttention/index.html">
<meta property="og:site_name" content="Albert&#39;s Resume">
<meta property="og:description" content=".red {   color: #DC3724; }   Introduction &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;This thesis mainly develops a supervised absolute  camera pose learning framework, in particular scenes,  takes a single RGB image as">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.imgur.com/g6Qqb2n.png">
<meta property="og:image" content="https://i.imgur.com/QFnRGZH.png">
<meta property="og:image" content="https://i.imgur.com/qN94zMP.png">
<meta property="og:image" content="https://i.imgur.com/w4PsABz.png">
<meta property="og:image" content="https://i.imgur.com/u92wBH0.png">
<meta property="og:image" content="https://i.imgur.com/47IFKRL.png">
<meta property="og:image" content="https://i.imgur.com/hnlzgRl.png">
<meta property="og:image" content="https://i.imgur.com/d9Dv3HY.png">
<meta property="og:image" content="https://i.imgur.com/ZbIKV4G.png">
<meta property="og:image" content="https://i.imgur.com/WYl0tjt.png">
<meta property="og:image" content="https://i.imgur.com/s30O0SR.png">
<meta property="article:published_time" content="2021-02-03T06:14:25.000Z">
<meta property="article:author" content="Albert Li">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/g6Qqb2n.png">

<link rel="canonical" href="http://example.com/PoseAttention/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>PoseAttention | Albert's Resume
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Albert's Resume</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-poseattention">

    <a href="/PoseAttention/" rel="section"><i class="fas fa-map-marker-alt fa-fw"></i>PoseAttention</a>

  </li>
        <li class="menu-item menu-item-taichiar">

    <a href="/TaiChiAR/" rel="section"><i class="fas fa-running fa-fw"></i>TaiChiAR</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">PoseAttention
</h1>

<div class="post-meta">
  

</div>

</header>

      
      
      
      <div class="post-body">
          <style>
.red {
  color: #DC3724;
}
</style>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><span class="red">Introduction</span></h2><font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;This thesis mainly develops a <b>supervised absolute 
camera pose learning framework</b>, in particular scenes, 
takes a single RGB image as input and output the 
predicted camera pose of the image. The whole <b>system is 
end-to-end with no need of additional mathematical 
optimization</b> to maintain the fast speed characteristic. 
Our system architecture is mainly based on <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1611.07890.pdf">PoseLSTM</a> by <b>incorporating a feature-weighted mechanism</b> to 
improve the unfair proportion between original features 
to predict a better position and rotation. In the loss function, 
we use the <b>learn weight loss function</b> which is 
proposed by <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.00390.pdf">Kendall, et al.</a> to save a lot of time and labor costs, and at the same time increase the accuracy of the loss function 
in describing the scale ratio.
</font>
<br>
<font size=4> ● Input Single Image and Output Camera Pose </font>
<p align="center">
  <img src="https://i.imgur.com/g6Qqb2n.png" width='545px' height='205px'/>
</p>

<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a><span class="red">Methodology</span></h2><font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;The LSTM is designed to learn the temporal information, which makes <b>the proportion of the feature vector be positively related to whether its position is close to the output position</b>.
It is reasonable on time-related sequence. But the issue PoseLSTM dealing with is not. The vectors have no time-related relationships.
</font>
<br>
<font size=4> ● Unfair Proportion in LSTM </font>
<p align="center">
  <img src="https://i.imgur.com/QFnRGZH.png" width='650px' height='360px'/>
</p>
<font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;A reasonable solution is to discard the original output, use the outputted feature vectors of LSTM cells at each point of sequence. Calculate the scores of each, then apply inner product to those outputted vectors and scores, and add up all of them as a new outputted hidden vector. In this way, we can <b>give each vector a different degree of importance</b>.
</font>
<br>
<font size=4> ● Give different degree of importance </font>
<p align="center">
  <img src="https://i.imgur.com/qN94zMP.png" width='633px' height='324px'/>
</p>

<font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;
We divide the entire architecture into two big blocks for analysis: <b>edited PoseLSTM</b> and <b>feature-weighted mechanism</b>. The query image first passed through the edited PoseLSTM, then generate four kids of outputted feature vectors which come from each LSTM cells in four directions respectively. And these four kinds of feature 
vectors keep feed-forwarding through our feature weighted mechanism then produce 6DoF camera pose.
</font>
<br>
<font size=4> ● Architecture of PoseAttention </font>
<p align="center">
  <img src="https://i.imgur.com/w4PsABz.png" width='777px' height='172px'/>
</p>

<font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;
When passing edited PoseLSTM, image needs to go through GoogLeNet first, 
and generates a 2048 high-dimensional feature 
vector, y, describing the input image. Next, we resizes the 
2048 dimensional feature vector to a 2D feature map, Y, 
with the size of 32*64 and takes this 2D feature map as 
the input of four LSTMs according to four directions 
which are the up, down, left and right. So far it has done 
the same as PoseLSTM does. <b>The difference is that we edited the output of these 
four LSTMs. We discard the hidden state vector of the 
last sequence point, and conversely record the output of 
each LSTM cells, y’</b>.
</font>
<br>
<font size=4> ● Edited PoseLSTM </font>
<p align="center">
  <img src="https://i.imgur.com/u92wBH0.png" width='724x' height='260px'/>
</p>
<font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;
In this part, <b>feature-weighted mechanism plays the 
role of predicting, adjusting and fine-tuning the weights 
of those feature vectors</b>. First, we concatenate the output feature vectors, y’, 
in the left-right direction and up-down direction 
respectively, in order to <b>let the concatenated feature 
vectors, y", have the cognition of bi-directional 
information</b> to better learn their proportion. Then, feeds 
the concatenated feature vectors into a fully connected 
layer which <b>compress the 64 dimensional feature vectors 
to 1 dimensional proportion scores</b>. Finally, apply inner product to these scores 
and the original concatenated feature vectors, and add up 
all feature vectors to generate a weighted feature vector.
</font>
<br>
<font size=4> ● Feature-weighted Mechanism </font>
<p align="center">
  <img src="https://i.imgur.com/47IFKRL.png" width='840px' height='264px'/>
</p>
<font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;
We use learn weight loss function which 
<b>takes Sx and Sq  as parameters to learn the different scales between positions and rotatioins</b>.
The benefits of this loss function is that the 
<b>automatically tuning characteristic</b>, which save human 
tuning cost and also let the precision of the scale ratio to be more accurate. It is because the automatically tuning 
may achieve the accuracy below decimal point rather 
than human tuning which may only adjust between 
limited numbers.
</font>
<br>
<font size=4> ● Learn Weight Loss Function </font>
<p align="center">
  <img src="https://i.imgur.com/hnlzgRl.png" width='740px' height='77px'/>
</p>

<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a><span class="red">Datasets</span></h2><font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;
The outdoor dataset is Cambridge Landmarks, which is 
mainly an outdoor street view near Cambridge University. 
It is a large-scale RGB dataset, provided by <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf">PoseNet</a> in ICCV 2015, covers an area of 50000 square 
meters, and <b>contains four scenes, Kings College, Old 
Hospital, Shop Façade and St Mary’s Church</b>. The RGB 
images are recorded by the mobile phone camera, 
including about 8000 images for training and 5000 
images for testing, and the <b>ground truth of these images 
are obtained by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Structure_from_motion">structure-from-motion</a></b>.
</font>
<br>
<font size=4> ● Outdoor Dataset - Cambridge Landmarks </font>
<p align="center">
  <img src="https://i.imgur.com/d9Dv3HY.png" width='783px' height='120px'/>
</p>
<font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;
The indoor dataset is Seven Scenes, which is mainly the scenes 
of indoor office, provided by Microsoft in 2013. It is a 
small-scale RGB-D dataset, covers an area of 12 square 
meters, and <b>contains seven scenes, chess, fire, heads, 
office, pumpkin, red kitchen, stairs</b>. The RGB-D images 
are recorded by handheld Kinect RGB-D camera, 
including 26000 images for training and 17000 images 
for testing, and the <b>ground truth are obtained by <a herf="https://dl.acm.org/doi/pdf/10.1145/2047196.2047270?casa_token=jzvqHBIRADQAAAAA:kJSJRxO4XWANUN_KQQ1fGYqZ3ZVu4uHvrV7s9T6H3k5h-yV0FJcNL84rQ9wo2FPkCd3HXjNDmFy_gEY">Kinect 
Fusion system</a></b>. Although this is an RGB-D dataset, 
we only use the RGB data for experiments, without using 
depth.
</font>
<br>
<font size=4> ● Indoor Dataset - Seven Scenes </font>
<p align="center">
  <img src="https://i.imgur.com/ZbIKV4G.png" width='804px' height='108px'/>
</p>

<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><span class="red">Experiments</span></h2><font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;
The data we presented are mainly 
the median error of PoseNet, PoseLSTM and our system, 
where the <b>data in the parentheses in last two columns is 
the percentage of progress relative to the results of 
PoseNet</b>. It can be clearly seen from the table that <b>our 
median error is much smaller than PoseNet and 
PoseLSTM</b>, outperforms PoseNet about 46% on position 
and 44% on rotation, and also outperforms PoseLSTM 
about 14% on position and 31% on rotation.
</font>
<br>
<font size=4> ● Median Error of Cambridge Landmarks </font>
<p align="center">
  <img src="https://i.imgur.com/WYl0tjt.png" width='1000px' height='226px'/>
</p>
<font size=4> ● Median Error of Seven Scenes </font>
<p align="center">
  <img src="https://i.imgur.com/s30O0SR.png" width='1000px' height='300px'/>
</p>

<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a><span class="red">Conclusions</span></h2><font size=3>
&nbsp;&nbsp;&nbsp;&nbsp;
Our first contribution is <b>showing that feature weighted mechanism in LSTM is a viable method</b> on 
localization. Second contribution is <b>outperforming 
PoseNet and PoseLSTM on localization accuracy</b>. We 
outperform PoseNet about 46% on position and 44% on 
rotation in outdoor scenes, and outperform 51% on 
position and 28% on rotation in indoor scenes. Also, we 
outperform PoseLSTM about 17% on position and 27% 
on rotation in outdoor scenes, and outperform 43% on 
position and 22% on rotation in indoor scenes. Third 
contribution is that we <b>maintain the characteristic of real time running</b>, which reaches 90 fps in our experiments, 
and it only needs the fixed memory when operating.
</font>

<h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a><span class="red">Demo</span></h2><p><font size=4> ● Trajectory Visualization of St Marys Church </font></p>
<iframe src='https://gfycat.com/ifr/FastFrigidAbalone' frameborder='0' scrolling='no' allowfullscreen width='960' height='361'></iframe>

      </div>
      
      
      
    </div>
    

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Methodology"><span class="nav-number">2.</span> <span class="nav-text">Methodology</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Datasets"><span class="nav-number">3.</span> <span class="nav-text">Datasets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiments"><span class="nav-number">4.</span> <span class="nav-text">Experiments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusions"><span class="nav-number">5.</span> <span class="nav-text">Conclusions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Demo"><span class="nav-number">6.</span> <span class="nav-text">Demo</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Albert Li"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Albert Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/s4816577" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;s4816577" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:b03902029@gmail.com" title="E-Mail → mailto:b03902029@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/chun-hsien-li-3107581a6/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;chun-hsien-li-3107581a6&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/profile.php?id=100000536735319" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;profile.php?id&#x3D;100000536735319" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-keyboard"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Albert Li</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
